{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function replace in module pandas.core.frame:\n",
      "\n",
      "replace(self, to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad')\n",
      "    Replace values given in `to_replace` with `value`.\n",
      "    \n",
      "    Values of the DataFrame are replaced with other values dynamically.\n",
      "    This differs from updating with ``.loc`` or ``.iloc``, which require\n",
      "    you to specify a location to update with some value.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    to_replace : str, regex, list, dict, Series, int, float, or None\n",
      "        How to find the values that will be replaced.\n",
      "    \n",
      "        * numeric, str or regex:\n",
      "    \n",
      "            - numeric: numeric values equal to `to_replace` will be\n",
      "              replaced with `value`\n",
      "            - str: string exactly matching `to_replace` will be replaced\n",
      "              with `value`\n",
      "            - regex: regexs matching `to_replace` will be replaced with\n",
      "              `value`\n",
      "    \n",
      "        * list of str, regex, or numeric:\n",
      "    \n",
      "            - First, if `to_replace` and `value` are both lists, they\n",
      "              **must** be the same length.\n",
      "            - Second, if ``regex=True`` then all of the strings in **both**\n",
      "              lists will be interpreted as regexs otherwise they will match\n",
      "              directly. This doesn't matter much for `value` since there\n",
      "              are only a few possible substitution regexes you can use.\n",
      "            - str, regex and numeric rules apply as above.\n",
      "    \n",
      "        * dict:\n",
      "    \n",
      "            - Dicts can be used to specify different replacement values\n",
      "              for different existing values. For example,\n",
      "              ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n",
      "              'y' with 'z'. To use a dict in this way the `value`\n",
      "              parameter should be `None`.\n",
      "            - For a DataFrame a dict can specify that different values\n",
      "              should be replaced in different columns. For example,\n",
      "              ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n",
      "              and the value 'z' in column 'b' and replaces these values\n",
      "              with whatever is specified in `value`. The `value` parameter\n",
      "              should not be ``None`` in this case. You can treat this as a\n",
      "              special case of passing two lists except that you are\n",
      "              specifying the column to search in.\n",
      "            - For a DataFrame nested dictionaries, e.g.,\n",
      "              ``{'a': {'b': np.nan}}``, are read as follows: look in column\n",
      "              'a' for the value 'b' and replace it with NaN. The `value`\n",
      "              parameter should be ``None`` to use a nested dict in this\n",
      "              way. You can nest regular expressions as well. Note that\n",
      "              column names (the top-level dictionary keys in a nested\n",
      "              dictionary) **cannot** be regular expressions.\n",
      "    \n",
      "        * None:\n",
      "    \n",
      "            - This means that the `regex` argument must be a string,\n",
      "              compiled regular expression, or list, dict, ndarray or\n",
      "              Series of such elements. If `value` is also ``None`` then\n",
      "              this **must** be a nested dictionary or Series.\n",
      "    \n",
      "        See the examples section for examples of each of these.\n",
      "    value : scalar, dict, list, str, regex, default None\n",
      "        Value to replace any values matching `to_replace` with.\n",
      "        For a DataFrame a dict of values can be used to specify which\n",
      "        value to use for each column (columns not in the dict will not be\n",
      "        filled). Regular expressions, strings and lists or dicts of such\n",
      "        objects are also allowed.\n",
      "    inplace : bool, default False\n",
      "        If True, in place. Note: this will modify any\n",
      "        other views on this object (e.g. a column from a DataFrame).\n",
      "        Returns the caller if this is True.\n",
      "    limit : int, default None\n",
      "        Maximum size gap to forward or backward fill.\n",
      "    regex : bool or same types as `to_replace`, default False\n",
      "        Whether to interpret `to_replace` and/or `value` as regular\n",
      "        expressions. If this is ``True`` then `to_replace` *must* be a\n",
      "        string. Alternatively, this could be a regular expression or a\n",
      "        list, dict, or array of regular expressions in which case\n",
      "        `to_replace` must be ``None``.\n",
      "    method : {'pad', 'ffill', 'bfill', `None`}\n",
      "        The method to use when for replacement, when `to_replace` is a\n",
      "        scalar, list or tuple and `value` is ``None``.\n",
      "    \n",
      "        .. versionchanged:: 0.23.0\n",
      "            Added to DataFrame.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        Object after replacement.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    AssertionError\n",
      "        * If `regex` is not a ``bool`` and `to_replace` is not\n",
      "          ``None``.\n",
      "    \n",
      "    TypeError\n",
      "        * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n",
      "        * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n",
      "          ``dict``, ``ndarray``, or ``Series``\n",
      "        * If `to_replace` is ``None`` and `regex` is not compilable\n",
      "          into a regular expression or is a list, dict, ndarray, or\n",
      "          Series.\n",
      "        * When replacing multiple ``bool`` or ``datetime64`` objects and\n",
      "          the arguments to `to_replace` does not match the type of the\n",
      "          value being replaced\n",
      "    \n",
      "    ValueError\n",
      "        * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n",
      "          `value` but they are not the same length.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.fillna : Fill NA values.\n",
      "    DataFrame.where : Replace values based on boolean condition.\n",
      "    Series.str.replace : Simple string replacement.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    * Regex substitution is performed under the hood with ``re.sub``. The\n",
      "      rules for substitution for ``re.sub`` are the same.\n",
      "    * Regular expressions will only substitute on strings, meaning you\n",
      "      cannot provide, for example, a regular expression matching floating\n",
      "      point numbers and expect the columns in your frame that have a\n",
      "      numeric dtype to be matched. However, if those floating point\n",
      "      numbers *are* strings, then you can do this.\n",
      "    * This method has *a lot* of options. You are encouraged to experiment\n",
      "      and play with this method to gain intuition about how it works.\n",
      "    * When dict is used as the `to_replace` value, it is like\n",
      "      key(s) in the dict are the to_replace part and\n",
      "      value(s) in the dict are the value parameter.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    **Scalar `to_replace` and `value`**\n",
      "    \n",
      "    >>> s = pd.Series([0, 1, 2, 3, 4])\n",
      "    >>> s.replace(0, 5)\n",
      "    0    5\n",
      "    1    1\n",
      "    2    2\n",
      "    3    3\n",
      "    4    4\n",
      "    dtype: int64\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n",
      "    ...                    'B': [5, 6, 7, 8, 9],\n",
      "    ...                    'C': ['a', 'b', 'c', 'd', 'e']})\n",
      "    >>> df.replace(0, 5)\n",
      "       A  B  C\n",
      "    0  5  5  a\n",
      "    1  1  6  b\n",
      "    2  2  7  c\n",
      "    3  3  8  d\n",
      "    4  4  9  e\n",
      "    \n",
      "    **List-like `to_replace`**\n",
      "    \n",
      "    >>> df.replace([0, 1, 2, 3], 4)\n",
      "       A  B  C\n",
      "    0  4  5  a\n",
      "    1  4  6  b\n",
      "    2  4  7  c\n",
      "    3  4  8  d\n",
      "    4  4  9  e\n",
      "    \n",
      "    >>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n",
      "       A  B  C\n",
      "    0  4  5  a\n",
      "    1  3  6  b\n",
      "    2  2  7  c\n",
      "    3  1  8  d\n",
      "    4  4  9  e\n",
      "    \n",
      "    >>> s.replace([1, 2], method='bfill')\n",
      "    0    0\n",
      "    1    3\n",
      "    2    3\n",
      "    3    3\n",
      "    4    4\n",
      "    dtype: int64\n",
      "    \n",
      "    **dict-like `to_replace`**\n",
      "    \n",
      "    >>> df.replace({0: 10, 1: 100})\n",
      "         A  B  C\n",
      "    0   10  5  a\n",
      "    1  100  6  b\n",
      "    2    2  7  c\n",
      "    3    3  8  d\n",
      "    4    4  9  e\n",
      "    \n",
      "    >>> df.replace({'A': 0, 'B': 5}, 100)\n",
      "         A    B  C\n",
      "    0  100  100  a\n",
      "    1    1    6  b\n",
      "    2    2    7  c\n",
      "    3    3    8  d\n",
      "    4    4    9  e\n",
      "    \n",
      "    >>> df.replace({'A': {0: 100, 4: 400}})\n",
      "         A  B  C\n",
      "    0  100  5  a\n",
      "    1    1  6  b\n",
      "    2    2  7  c\n",
      "    3    3  8  d\n",
      "    4  400  9  e\n",
      "    \n",
      "    **Regular expression `to_replace`**\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n",
      "    ...                    'B': ['abc', 'bar', 'xyz']})\n",
      "    >>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n",
      "          A    B\n",
      "    0   new  abc\n",
      "    1   foo  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n",
      "          A    B\n",
      "    0   new  abc\n",
      "    1   foo  bar\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace(regex=r'^ba.$', value='new')\n",
      "          A    B\n",
      "    0   new  abc\n",
      "    1   foo  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n",
      "          A    B\n",
      "    0   new  abc\n",
      "    1   xyz  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    >>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n",
      "          A    B\n",
      "    0   new  abc\n",
      "    1   new  new\n",
      "    2  bait  xyz\n",
      "    \n",
      "    Note that when replacing multiple ``bool`` or ``datetime64`` objects,\n",
      "    the data types in the `to_replace` parameter must match the data\n",
      "    type of the value being replaced:\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': [True, False, True],\n",
      "    ...                    'B': [False, True, False]})\n",
      "    >>> df.replace({'a string': 'new value', True: False})  # raises\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    TypeError: Cannot compare types 'ndarray(dtype=bool)' and 'str'\n",
      "    \n",
      "    This raises a ``TypeError`` because one of the ``dict`` keys is not of\n",
      "    the correct type for replacement.\n",
      "    \n",
      "    Compare the behavior of ``s.replace({'a': None})`` and\n",
      "    ``s.replace('a', None)`` to understand the peculiarities\n",
      "    of the `to_replace` parameter:\n",
      "    \n",
      "    >>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n",
      "    \n",
      "    When one uses a dict as the `to_replace` value, it is like the\n",
      "    value(s) in the dict are equal to the `value` parameter.\n",
      "    ``s.replace({'a': None})`` is equivalent to\n",
      "    ``s.replace(to_replace={'a': None}, value=None, method=None)``:\n",
      "    \n",
      "    >>> s.replace({'a': None})\n",
      "    0      10\n",
      "    1    None\n",
      "    2    None\n",
      "    3       b\n",
      "    4    None\n",
      "    dtype: object\n",
      "    \n",
      "    When ``value=None`` and `to_replace` is a scalar, list or\n",
      "    tuple, `replace` uses the method parameter (default 'pad') to do the\n",
      "    replacement. So this is why the 'a' values are being replaced by 10\n",
      "    in rows 1 and 2 and 'b' in row 4 in this case.\n",
      "    The command ``s.replace('a', None)`` is actually equivalent to\n",
      "    ``s.replace(to_replace='a', value=None, method='pad')``:\n",
      "    \n",
      "    >>> s.replace('a', None)\n",
      "    0    10\n",
      "    1    10\n",
      "    2    10\n",
      "    3     b\n",
      "    4     b\n",
      "    dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module string:\n",
      "\n",
      "NAME\n",
      "    string - A collection of string constants.\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.8/library/string\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    Public module variables:\n",
      "    \n",
      "    whitespace -- a string containing all ASCII whitespace\n",
      "    ascii_lowercase -- a string containing all ASCII lowercase letters\n",
      "    ascii_uppercase -- a string containing all ASCII uppercase letters\n",
      "    ascii_letters -- a string containing all ASCII letters\n",
      "    digits -- a string containing all ASCII decimal digits\n",
      "    hexdigits -- a string containing all ASCII hexadecimal digits\n",
      "    octdigits -- a string containing all ASCII octal digits\n",
      "    punctuation -- a string containing all ASCII punctuation characters\n",
      "    printable -- a string containing all ASCII characters considered printable\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Formatter\n",
      "        Template\n",
      "    \n",
      "    class Formatter(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  check_unused_args(self, used_args, args, kwargs)\n",
      "     |  \n",
      "     |  convert_field(self, value, conversion)\n",
      "     |  \n",
      "     |  format(self, format_string, /, *args, **kwargs)\n",
      "     |  \n",
      "     |  format_field(self, value, format_spec)\n",
      "     |  \n",
      "     |  get_field(self, field_name, args, kwargs)\n",
      "     |      # given a field_name, find the object it references.\n",
      "     |      #  field_name:   the field being looked up, e.g. \"0.name\"\n",
      "     |      #                 or \"lookup[3]\"\n",
      "     |      #  used_args:    a set of which args have been used\n",
      "     |      #  args, kwargs: as passed in to vformat\n",
      "     |  \n",
      "     |  get_value(self, key, args, kwargs)\n",
      "     |  \n",
      "     |  parse(self, format_string)\n",
      "     |      # returns an iterable that contains tuples of the form:\n",
      "     |      # (literal_text, field_name, format_spec, conversion)\n",
      "     |      # literal_text can be zero length\n",
      "     |      # field_name can be None, in which case there's no\n",
      "     |      #  object to format and output\n",
      "     |      # if field_name is not None, it is looked up, formatted\n",
      "     |      #  with format_spec and conversion and then used\n",
      "     |  \n",
      "     |  vformat(self, format_string, args, kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Template(builtins.object)\n",
      "     |  Template(template)\n",
      "     |  \n",
      "     |  A string class for supporting $-substitutions.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, template)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  safe_substitute(self, mapping={}, /, **kws)\n",
      "     |  \n",
      "     |  substitute(self, mapping={}, /, **kws)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  braceidpattern = None\n",
      "     |  \n",
      "     |  delimiter = '$'\n",
      "     |  \n",
      "     |  flags = re.IGNORECASE\n",
      "     |  \n",
      "     |  idpattern = '(?a:[_a-z][_a-z0-9]*)'\n",
      "     |  \n",
      "     |  pattern = re.compile('\\n    \\\\$(?:\\n      (?P<escaped>\\\\$)...ced>(?a:[...\n",
      "\n",
      "FUNCTIONS\n",
      "    capwords(s, sep=None)\n",
      "        capwords(s [,sep]) -> string\n",
      "        \n",
      "        Split the argument into words using split, capitalize each\n",
      "        word using capitalize, and join the capitalized words using\n",
      "        join.  If the optional second argument sep is absent or None,\n",
      "        runs of whitespace characters are replaced by a single space\n",
      "        and leading and trailing whitespace are removed, otherwise\n",
      "        sep is used to split and join the words.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ascii_letters', 'ascii_lowercase', 'ascii_uppercase', 'cap...\n",
      "    ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "    ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n",
      "    ascii_uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "    digits = '0123456789'\n",
      "    hexdigits = '0123456789abcdefABCDEF'\n",
      "    octdigits = '01234567'\n",
      "    printable = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTU...\n",
      "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
      "    whitespace = ' \\t\\n\\r\\x0b\\x0c'\n",
      "\n",
      "FILE\n",
      "    /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/string.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "help(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a is b'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "x = 'a is b (not c)'\n",
    "re.sub(' \\(.+\\)', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function drop in module pandas.core.frame:\n",
      "\n",
      "drop(self, labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
      "    Drop specified labels from rows or columns.\n",
      "    \n",
      "    Remove rows or columns by specifying label names and corresponding\n",
      "    axis, or by specifying directly index or column names. When using a\n",
      "    multi-index, labels on different levels can be removed by specifying\n",
      "    the level.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    labels : single label or list-like\n",
      "        Index or column labels to drop.\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "        Whether to drop labels from the index (0 or 'index') or\n",
      "        columns (1 or 'columns').\n",
      "    index : single label or list-like\n",
      "        Alternative to specifying axis (``labels, axis=0``\n",
      "        is equivalent to ``index=labels``).\n",
      "    columns : single label or list-like\n",
      "        Alternative to specifying axis (``labels, axis=1``\n",
      "        is equivalent to ``columns=labels``).\n",
      "    level : int or level name, optional\n",
      "        For MultiIndex, level from which the labels will be removed.\n",
      "    inplace : bool, default False\n",
      "        If False, return a copy. Otherwise, do operation\n",
      "        inplace and return None.\n",
      "    errors : {'ignore', 'raise'}, default 'raise'\n",
      "        If 'ignore', suppress error and only existing labels are\n",
      "        dropped.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        DataFrame without the removed index or column labels.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    KeyError\n",
      "        If any of the labels is not found in the selected axis.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.loc : Label-location based indexer for selection by label.\n",
      "    DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
      "        where (all or any) data are missing.\n",
      "    DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
      "        removed, optionally only considering certain columns.\n",
      "    Series.drop : Return Series with specified index labels removed.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n",
      "    ...                   columns=['A', 'B', 'C', 'D'])\n",
      "    >>> df\n",
      "       A  B   C   D\n",
      "    0  0  1   2   3\n",
      "    1  4  5   6   7\n",
      "    2  8  9  10  11\n",
      "    \n",
      "    Drop columns\n",
      "    \n",
      "    >>> df.drop(['B', 'C'], axis=1)\n",
      "       A   D\n",
      "    0  0   3\n",
      "    1  4   7\n",
      "    2  8  11\n",
      "    \n",
      "    >>> df.drop(columns=['B', 'C'])\n",
      "       A   D\n",
      "    0  0   3\n",
      "    1  4   7\n",
      "    2  8  11\n",
      "    \n",
      "    Drop a row by index\n",
      "    \n",
      "    >>> df.drop([0, 1])\n",
      "       A  B   C   D\n",
      "    2  8  9  10  11\n",
      "    \n",
      "    Drop columns and/or rows of MultiIndex DataFrame\n",
      "    \n",
      "    >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n",
      "    ...                              ['speed', 'weight', 'length']],\n",
      "    ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
      "    ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
      "    >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n",
      "    ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
      "    ...                         [250, 150], [1.5, 0.8], [320, 250],\n",
      "    ...                         [1, 0.8], [0.3, 0.2]])\n",
      "    >>> df\n",
      "                    big     small\n",
      "    lama    speed   45.0    30.0\n",
      "            weight  200.0   100.0\n",
      "            length  1.5     1.0\n",
      "    cow     speed   30.0    20.0\n",
      "            weight  250.0   150.0\n",
      "            length  1.5     0.8\n",
      "    falcon  speed   320.0   250.0\n",
      "            weight  1.0     0.8\n",
      "            length  0.3     0.2\n",
      "    \n",
      "    >>> df.drop(index='cow', columns='small')\n",
      "                    big\n",
      "    lama    speed   45.0\n",
      "            weight  200.0\n",
      "            length  1.5\n",
      "    falcon  speed   320.0\n",
      "            weight  1.0\n",
      "            length  0.3\n",
      "    \n",
      "    >>> df.drop(index='length', level=1)\n",
      "                    big     small\n",
      "    lama    speed   45.0    30.0\n",
      "            weight  200.0   100.0\n",
      "    cow     speed   30.0    20.0\n",
      "            weight  250.0   150.0\n",
      "    falcon  speed   320.0   250.0\n",
      "            weight  1.0     0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handler (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparseable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned.\n",
      "    warn_bad_lines : bool, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are `None` for the ordinary converter,\n",
      "        `high` for the high-precision converter, and `round_trip` for the\n",
      "        round-trip converter.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function merge in module pandas.core.frame:\n",
      "\n",
      "merge(self, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None) -> 'DataFrame'\n",
      "    Merge DataFrame or named Series objects with a database-style join.\n",
      "    \n",
      "    The join is done on columns or indexes. If joining columns on\n",
      "    columns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\n",
      "    on indexes or indexes on a column or columns, the index will be passed on.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    right : DataFrame or named Series\n",
      "        Object to merge with.\n",
      "    how : {'left', 'right', 'outer', 'inner'}, default 'inner'\n",
      "        Type of merge to be performed.\n",
      "    \n",
      "        * left: use only keys from left frame, similar to a SQL left outer join;\n",
      "          preserve key order.\n",
      "        * right: use only keys from right frame, similar to a SQL right outer join;\n",
      "          preserve key order.\n",
      "        * outer: use union of keys from both frames, similar to a SQL full outer\n",
      "          join; sort keys lexicographically.\n",
      "        * inner: use intersection of keys from both frames, similar to a SQL inner\n",
      "          join; preserve the order of the left keys.\n",
      "    on : label or list\n",
      "        Column or index level names to join on. These must be found in both\n",
      "        DataFrames. If `on` is None and not merging on indexes then this defaults\n",
      "        to the intersection of the columns in both DataFrames.\n",
      "    left_on : label or list, or array-like\n",
      "        Column or index level names to join on in the left DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the left DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    right_on : label or list, or array-like\n",
      "        Column or index level names to join on in the right DataFrame. Can also\n",
      "        be an array or list of arrays of the length of the right DataFrame.\n",
      "        These arrays are treated as if they are columns.\n",
      "    left_index : bool, default False\n",
      "        Use the index from the left DataFrame as the join key(s). If it is a\n",
      "        MultiIndex, the number of keys in the other DataFrame (either the index\n",
      "        or a number of columns) must match the number of levels.\n",
      "    right_index : bool, default False\n",
      "        Use the index from the right DataFrame as the join key. Same caveats as\n",
      "        left_index.\n",
      "    sort : bool, default False\n",
      "        Sort the join keys lexicographically in the result DataFrame. If False,\n",
      "        the order of the join keys depends on the join type (how keyword).\n",
      "    suffixes : list-like, default is (\"_x\", \"_y\")\n",
      "        A length-2 sequence where each element is optionally a string\n",
      "        indicating the suffix to add to overlapping column names in\n",
      "        `left` and `right` respectively. Pass a value of `None` instead\n",
      "        of a string to indicate that the column name from `left` or\n",
      "        `right` should be left as-is, with no suffix. At least one of the\n",
      "        values must not be None.\n",
      "    copy : bool, default True\n",
      "        If False, avoid copy if possible.\n",
      "    indicator : bool or str, default False\n",
      "        If True, adds a column to the output DataFrame called \"_merge\" with\n",
      "        information on the source of each row. The column can be given a different\n",
      "        name by providing a string argument. The column will have a Categorical\n",
      "        type with the value of \"left_only\" for observations whose merge key only\n",
      "        appears in the left DataFrame, \"right_only\" for observations\n",
      "        whose merge key only appears in the right DataFrame, and \"both\"\n",
      "        if the observation's merge key is found in both DataFrames.\n",
      "    \n",
      "    validate : str, optional\n",
      "        If specified, checks if merge is of specified type.\n",
      "    \n",
      "        * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n",
      "          left and right datasets.\n",
      "        * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n",
      "          dataset.\n",
      "        * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n",
      "          dataset.\n",
      "        * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        A DataFrame of the two merged objects.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    merge_ordered : Merge with optional filling/interpolation.\n",
      "    merge_asof : Merge on nearest keys.\n",
      "    DataFrame.join : Similar method using indices.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Support for specifying index levels as the `on`, `left_on`, and\n",
      "    `right_on` parameters was added in version 0.23.0\n",
      "    Support for merging named Series objects was added in version 0.24.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n",
      "    ...                     'value': [1, 2, 3, 5]})\n",
      "    >>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n",
      "    ...                     'value': [5, 6, 7, 8]})\n",
      "    >>> df1\n",
      "        lkey value\n",
      "    0   foo      1\n",
      "    1   bar      2\n",
      "    2   baz      3\n",
      "    3   foo      5\n",
      "    >>> df2\n",
      "        rkey value\n",
      "    0   foo      5\n",
      "    1   bar      6\n",
      "    2   baz      7\n",
      "    3   foo      8\n",
      "    \n",
      "    Merge df1 and df2 on the lkey and rkey columns. The value columns have\n",
      "    the default suffixes, _x and _y, appended.\n",
      "    \n",
      "    >>> df1.merge(df2, left_on='lkey', right_on='rkey')\n",
      "      lkey  value_x rkey  value_y\n",
      "    0  foo        1  foo        5\n",
      "    1  foo        1  foo        8\n",
      "    2  foo        5  foo        5\n",
      "    3  foo        5  foo        8\n",
      "    4  bar        2  bar        6\n",
      "    5  baz        3  baz        7\n",
      "    \n",
      "    Merge DataFrames df1 and df2 with specified left and right suffixes\n",
      "    appended to any overlapping columns.\n",
      "    \n",
      "    >>> df1.merge(df2, left_on='lkey', right_on='rkey',\n",
      "    ...           suffixes=('_left', '_right'))\n",
      "      lkey  value_left rkey  value_right\n",
      "    0  foo           1  foo            5\n",
      "    1  foo           1  foo            8\n",
      "    2  foo           5  foo            5\n",
      "    3  foo           5  foo            8\n",
      "    4  bar           2  bar            6\n",
      "    5  baz           3  baz            7\n",
      "    \n",
      "    Merge DataFrames df1 and df2, but raise an exception if the DataFrames have\n",
      "    any overlapping columns.\n",
      "    \n",
      "    >>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: columns overlap but no suffix specified:\n",
      "        Index(['value'], dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ib</th>\n",
       "      <th>II</th>\n",
       "      <th>III</th>\n",
       "      <th>IV</th>\n",
       "      <th>V</th>\n",
       "      <th>VI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>60</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ib  II  III  IV   V    VI\n",
       "a3    3   9   12  15  18   NaN\n",
       "b5    5  10   15  20  25  30.0\n",
       "c8    8  16   24  32  40  48.0\n",
       "d12  12  24   36  48  60  72.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.DataFrame([[3, 9, 12, 15, 18], \n",
    "                    [5, 10, 15, 20, 25, 30], \n",
    "                    [8, 16, 24, 32, 40, 48], \n",
    "                    [12, 24, 36, 48, 60, 72]], \n",
    "                  index=['a3', 'b5', 'c8', 'd12'],\n",
    "                  columns=['Ib', 'II', 'III', 'IV', 'V', 'VI'])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>II</th>\n",
       "      <th>III</th>\n",
       "      <th>IV</th>\n",
       "      <th>V</th>\n",
       "      <th>VI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>60</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      I  II  III  IV   V    VI\n",
       "a3    3   9   12  15  18   NaN\n",
       "b5    5  10   15  20  25  30.0\n",
       "c8    8  16   24  32  40  48.0\n",
       "d12  12  24   36  48  60  72.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns = [x.strip('b') for x in df5.columns]\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>VI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5</th>\n",
       "      <td>5</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8</th>\n",
       "      <td>8</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>12</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      I    VI\n",
       "a3    3   NaN\n",
       "b5    5  30.0\n",
       "c8    8  48.0\n",
       "d12  12  72.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.drop(df5.loc[:, 'II':'V'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>II</th>\n",
       "      <th>III</th>\n",
       "      <th>IV</th>\n",
       "      <th>V</th>\n",
       "      <th>VI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c8</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>60</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      I  II  III  IV   V    VI\n",
       "a3    3   9   12  15  18   NaN\n",
       "b5    5  10   15  20  25  30.0\n",
       "c8    8  16   24  32  40  48.0\n",
       "d12  12  24   36  48  60  72.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function rename in module pandas.core.frame:\n",
      "\n",
      "rename(self, mapper=None, index=None, columns=None, axis=None, copy=True, inplace=False, level=None, errors='ignore')\n",
      "    Alter axes labels.\n",
      "    \n",
      "    Function / dict values must be unique (1-to-1). Labels not contained in\n",
      "    a dict / Series will be left as-is. Extra labels listed don't throw an\n",
      "    error.\n",
      "    \n",
      "    See the :ref:`user guide <basics.rename>` for more.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    mapper : dict-like or function\n",
      "        Dict-like or functions transformations to apply to\n",
      "        that axis' values. Use either ``mapper`` and ``axis`` to\n",
      "        specify the axis to target with ``mapper``, or ``index`` and\n",
      "        ``columns``.\n",
      "    index : dict-like or function\n",
      "        Alternative to specifying axis (``mapper, axis=0``\n",
      "        is equivalent to ``index=mapper``).\n",
      "    columns : dict-like or function\n",
      "        Alternative to specifying axis (``mapper, axis=1``\n",
      "        is equivalent to ``columns=mapper``).\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "        Axis to target with ``mapper``. Can be either the axis name\n",
      "        ('index', 'columns') or number (0, 1). The default is 'index'.\n",
      "    copy : bool, default True\n",
      "        Also copy underlying data.\n",
      "    inplace : bool, default False\n",
      "        Whether to return a new DataFrame. If True then value of copy is\n",
      "        ignored.\n",
      "    level : int or level name, default None\n",
      "        In case of a MultiIndex, only rename labels in the specified\n",
      "        level.\n",
      "    errors : {'ignore', 'raise'}, default 'ignore'\n",
      "        If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n",
      "        or `columns` contains labels that are not present in the Index\n",
      "        being transformed.\n",
      "        If 'ignore', existing keys will be renamed and extra keys will be\n",
      "        ignored.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        DataFrame with the renamed axis labels.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    KeyError\n",
      "        If any of the labels is not found in the selected axis and\n",
      "        \"errors='raise'\".\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.rename_axis : Set the name of the axis.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    ``DataFrame.rename`` supports two calling conventions\n",
      "    \n",
      "    * ``(index=index_mapper, columns=columns_mapper, ...)``\n",
      "    * ``(mapper, axis={'index', 'columns'}, ...)``\n",
      "    \n",
      "    We *highly* recommend using keyword arguments to clarify your\n",
      "    intent.\n",
      "    \n",
      "    Rename columns using a mapping:\n",
      "    \n",
      "    >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n",
      "    >>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n",
      "       a  c\n",
      "    0  1  4\n",
      "    1  2  5\n",
      "    2  3  6\n",
      "    \n",
      "    Rename index using a mapping:\n",
      "    \n",
      "    >>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n",
      "       A  B\n",
      "    x  1  4\n",
      "    y  2  5\n",
      "    z  3  6\n",
      "    \n",
      "    Cast index labels to a different type:\n",
      "    \n",
      "    >>> df.index\n",
      "    RangeIndex(start=0, stop=3, step=1)\n",
      "    >>> df.rename(index=str).index\n",
      "    Index(['0', '1', '2'], dtype='object')\n",
      "    \n",
      "    >>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\n",
      "    Traceback (most recent call last):\n",
      "    KeyError: ['C'] not found in axis\n",
      "    \n",
      "    Using axis-style parameters\n",
      "    \n",
      "    >>> df.rename(str.lower, axis='columns')\n",
      "       a  b\n",
      "    0  1  4\n",
      "    1  2  5\n",
      "    2  3  6\n",
      "    \n",
      "    >>> df.rename({1: 2, 2: 4}, axis='index')\n",
      "       A  B\n",
      "    0  1  4\n",
      "    2  2  5\n",
      "    4  3  6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a3', 16.5), ('b5', 25.0), ('c8', 40.0), ('d12', 60.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df5.apply(lambda x: np.nanmean(x['IV':'VI']), axis=1).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sort_values in module pandas.core.series:\n",
      "\n",
      "sort_values(self, axis=0, ascending=True, inplace: bool = False, kind: str = 'quicksort', na_position: str = 'last', ignore_index: bool = False, key: Union[Callable[[ForwardRef('Series')], Union[ForwardRef('Series'), ~AnyArrayLike]], NoneType] = None)\n",
      "    Sort by the values.\n",
      "    \n",
      "    Sort a Series in ascending or descending order by some\n",
      "    criterion.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    axis : {0 or 'index'}, default 0\n",
      "        Axis to direct sorting. The value 'index' is accepted for\n",
      "        compatibility with DataFrame.sort_values.\n",
      "    ascending : bool, default True\n",
      "        If True, sort values in ascending order, otherwise descending.\n",
      "    inplace : bool, default False\n",
      "        If True, perform operation in-place.\n",
      "    kind : {'quicksort', 'mergesort' or 'heapsort'}, default 'quicksort'\n",
      "        Choice of sorting algorithm. See also :func:`numpy.sort` for more\n",
      "        information. 'mergesort' is the only stable  algorithm.\n",
      "    na_position : {'first' or 'last'}, default 'last'\n",
      "        Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n",
      "        the end.\n",
      "    ignore_index : bool, default False\n",
      "        If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      "    \n",
      "        .. versionadded:: 1.0.0\n",
      "    \n",
      "    key : callable, optional\n",
      "        If not None, apply the key function to the series values\n",
      "        before sorting. This is similar to the `key` argument in the\n",
      "        builtin :meth:`sorted` function, with the notable difference that\n",
      "        this `key` function should be *vectorized*. It should expect a\n",
      "        ``Series`` and return an array-like.\n",
      "    \n",
      "        .. versionadded:: 1.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Series\n",
      "        Series ordered by values.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Series.sort_index : Sort by the Series indices.\n",
      "    DataFrame.sort_values : Sort DataFrame by the values along either axis.\n",
      "    DataFrame.sort_index : Sort DataFrame by indices.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> s = pd.Series([np.nan, 1, 3, 10, 5])\n",
      "    >>> s\n",
      "    0     NaN\n",
      "    1     1.0\n",
      "    2     3.0\n",
      "    3     10.0\n",
      "    4     5.0\n",
      "    dtype: float64\n",
      "    \n",
      "    Sort values ascending order (default behaviour)\n",
      "    \n",
      "    >>> s.sort_values(ascending=True)\n",
      "    1     1.0\n",
      "    2     3.0\n",
      "    4     5.0\n",
      "    3    10.0\n",
      "    0     NaN\n",
      "    dtype: float64\n",
      "    \n",
      "    Sort values descending order\n",
      "    \n",
      "    >>> s.sort_values(ascending=False)\n",
      "    3    10.0\n",
      "    4     5.0\n",
      "    2     3.0\n",
      "    1     1.0\n",
      "    0     NaN\n",
      "    dtype: float64\n",
      "    \n",
      "    Sort values inplace\n",
      "    \n",
      "    >>> s.sort_values(ascending=False, inplace=True)\n",
      "    >>> s\n",
      "    3    10.0\n",
      "    4     5.0\n",
      "    2     3.0\n",
      "    1     1.0\n",
      "    0     NaN\n",
      "    dtype: float64\n",
      "    \n",
      "    Sort values putting NAs first\n",
      "    \n",
      "    >>> s.sort_values(na_position='first')\n",
      "    0     NaN\n",
      "    1     1.0\n",
      "    2     3.0\n",
      "    4     5.0\n",
      "    3    10.0\n",
      "    dtype: float64\n",
      "    \n",
      "    Sort a series of strings\n",
      "    \n",
      "    >>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n",
      "    >>> s\n",
      "    0    z\n",
      "    1    b\n",
      "    2    d\n",
      "    3    a\n",
      "    4    c\n",
      "    dtype: object\n",
      "    \n",
      "    >>> s.sort_values()\n",
      "    3    a\n",
      "    1    b\n",
      "    4    c\n",
      "    2    d\n",
      "    0    z\n",
      "    dtype: object\n",
      "    \n",
      "    Sort using a key function. Your `key` function will be\n",
      "    given the ``Series`` of values and should return an array-like.\n",
      "    \n",
      "    >>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n",
      "    >>> s.sort_values()\n",
      "    1    B\n",
      "    3    D\n",
      "    0    a\n",
      "    2    c\n",
      "    4    e\n",
      "    dtype: object\n",
      "    >>> s.sort_values(key=lambda x: x.str.lower())\n",
      "    0    a\n",
      "    1    B\n",
      "    2    c\n",
      "    3    D\n",
      "    4    e\n",
      "    dtype: object\n",
      "    \n",
      "    NumPy ufuncs work well here. For example, we can\n",
      "    sort by the ``sin`` of the value\n",
      "    \n",
      "    >>> s = pd.Series([-4, -2, 0, 2, 4])\n",
      "    >>> s.sort_values(key=np.sin)\n",
      "    1   -2\n",
      "    4    4\n",
      "    2    0\n",
      "    0   -4\n",
      "    3    2\n",
      "    dtype: int64\n",
      "    \n",
      "    More complicated user-defined functions can be used,\n",
      "    as long as they expect a Series and return an array-like\n",
      "    \n",
      "    >>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n",
      "    0   -4\n",
      "    3    2\n",
      "    4    4\n",
      "    1   -2\n",
      "    2    0\n",
      "    dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.Series.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dropna in module pandas.core.frame:\n",
      "\n",
      "dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
      "    Remove missing values.\n",
      "    \n",
      "    See the :ref:`User Guide <missing_data>` for more on which values are\n",
      "    considered missing, and how to work with missing data.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "        Determine if rows or columns which contain missing values are\n",
      "        removed.\n",
      "    \n",
      "        * 0, or 'index' : Drop rows which contain missing values.\n",
      "        * 1, or 'columns' : Drop columns which contain missing value.\n",
      "    \n",
      "        .. versionchanged:: 1.0.0\n",
      "    \n",
      "           Pass tuple or list to drop on multiple axes.\n",
      "           Only a single axis is allowed.\n",
      "    \n",
      "    how : {'any', 'all'}, default 'any'\n",
      "        Determine if row or column is removed from DataFrame, when we have\n",
      "        at least one NA or all NA.\n",
      "    \n",
      "        * 'any' : If any NA values are present, drop that row or column.\n",
      "        * 'all' : If all values are NA, drop that row or column.\n",
      "    \n",
      "    thresh : int, optional\n",
      "        Require that many non-NA values.\n",
      "    subset : array-like, optional\n",
      "        Labels along other axis to consider, e.g. if you are dropping rows\n",
      "        these would be a list of columns to include.\n",
      "    inplace : bool, default False\n",
      "        If True, do operation inplace and return None.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        DataFrame with NA entries dropped from it.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.isna: Indicate missing values.\n",
      "    DataFrame.notna : Indicate existing (non-missing) values.\n",
      "    DataFrame.fillna : Replace missing values.\n",
      "    Series.dropna : Drop missing values.\n",
      "    Index.dropna : Drop missing indices.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n",
      "    ...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n",
      "    ...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n",
      "    ...                             pd.NaT]})\n",
      "    >>> df\n",
      "           name        toy       born\n",
      "    0    Alfred        NaN        NaT\n",
      "    1    Batman  Batmobile 1940-04-25\n",
      "    2  Catwoman   Bullwhip        NaT\n",
      "    \n",
      "    Drop the rows where at least one element is missing.\n",
      "    \n",
      "    >>> df.dropna()\n",
      "         name        toy       born\n",
      "    1  Batman  Batmobile 1940-04-25\n",
      "    \n",
      "    Drop the columns where at least one element is missing.\n",
      "    \n",
      "    >>> df.dropna(axis='columns')\n",
      "           name\n",
      "    0    Alfred\n",
      "    1    Batman\n",
      "    2  Catwoman\n",
      "    \n",
      "    Drop the rows where all elements are missing.\n",
      "    \n",
      "    >>> df.dropna(how='all')\n",
      "           name        toy       born\n",
      "    0    Alfred        NaN        NaT\n",
      "    1    Batman  Batmobile 1940-04-25\n",
      "    2  Catwoman   Bullwhip        NaT\n",
      "    \n",
      "    Keep only the rows with at least 2 non-NA values.\n",
      "    \n",
      "    >>> df.dropna(thresh=2)\n",
      "           name        toy       born\n",
      "    1    Batman  Batmobile 1940-04-25\n",
      "    2  Catwoman   Bullwhip        NaT\n",
      "    \n",
      "    Define in which columns to look for missing values.\n",
      "    \n",
      "    >>> df.dropna(subset=['name', 'born'])\n",
      "           name        toy       born\n",
      "    1    Batman  Batmobile 1940-04-25\n",
      "    \n",
      "    Keep the DataFrame with valid entries in the same variable.\n",
      "    \n",
      "    >>> df.dropna(inplace=True)\n",
      "    >>> df\n",
      "         name        toy       born\n",
      "    1  Batman  Batmobile 1940-04-25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function where in module pandas.core.generic:\n",
      "\n",
      "where(self, cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False)\n",
      "    Replace values where the condition is False.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    cond : bool Series/DataFrame, array-like, or callable\n",
      "        Where `cond` is True, keep the original value. Where\n",
      "        False, replace with corresponding value from `other`.\n",
      "        If `cond` is callable, it is computed on the Series/DataFrame and\n",
      "        should return boolean Series/DataFrame or array. The callable must\n",
      "        not change input Series/DataFrame (though pandas doesn't check it).\n",
      "    other : scalar, Series/DataFrame, or callable\n",
      "        Entries where `cond` is False are replaced with\n",
      "        corresponding value from `other`.\n",
      "        If other is callable, it is computed on the Series/DataFrame and\n",
      "        should return scalar or Series/DataFrame. The callable must not\n",
      "        change input Series/DataFrame (though pandas doesn't check it).\n",
      "    inplace : bool, default False\n",
      "        Whether to perform the operation in place on the data.\n",
      "    axis : int, default None\n",
      "        Alignment axis if needed.\n",
      "    level : int, default None\n",
      "        Alignment level if needed.\n",
      "    errors : str, {'raise', 'ignore'}, default 'raise'\n",
      "        Note that currently this parameter won't affect\n",
      "        the results and will always coerce to a suitable dtype.\n",
      "    \n",
      "        - 'raise' : allow exceptions to be raised.\n",
      "        - 'ignore' : suppress exceptions. On error return original object.\n",
      "    \n",
      "    try_cast : bool, default False\n",
      "        Try to cast the result back to the input type (if possible).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Same type as caller\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    :func:`DataFrame.mask` : Return an object of same shape as\n",
      "        self.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The where method is an application of the if-then idiom. For each\n",
      "    element in the calling DataFrame, if ``cond`` is ``True`` the\n",
      "    element is used; otherwise the corresponding element from the DataFrame\n",
      "    ``other`` is used.\n",
      "    \n",
      "    The signature for :func:`DataFrame.where` differs from\n",
      "    :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n",
      "    ``np.where(m, df1, df2)``.\n",
      "    \n",
      "    For further details and examples see the ``where`` documentation in\n",
      "    :ref:`indexing <indexing.where_mask>`.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> s = pd.Series(range(5))\n",
      "    >>> s.where(s > 0)\n",
      "    0    NaN\n",
      "    1    1.0\n",
      "    2    2.0\n",
      "    3    3.0\n",
      "    4    4.0\n",
      "    dtype: float64\n",
      "    \n",
      "    >>> s.mask(s > 0)\n",
      "    0    0.0\n",
      "    1    NaN\n",
      "    2    NaN\n",
      "    3    NaN\n",
      "    4    NaN\n",
      "    dtype: float64\n",
      "    \n",
      "    >>> s.where(s > 1, 10)\n",
      "    0    10\n",
      "    1    10\n",
      "    2    2\n",
      "    3    3\n",
      "    4    4\n",
      "    dtype: int64\n",
      "    \n",
      "    >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n",
      "    >>> df\n",
      "       A  B\n",
      "    0  0  1\n",
      "    1  2  3\n",
      "    2  4  5\n",
      "    3  6  7\n",
      "    4  8  9\n",
      "    >>> m = df % 3 == 0\n",
      "    >>> df.where(m, -df)\n",
      "       A  B\n",
      "    0  0 -1\n",
      "    1 -2  3\n",
      "    2 -4 -5\n",
      "    3  6 -7\n",
      "    4 -8  9\n",
      "    >>> df.where(m, -df) == np.where(m, df, -df)\n",
      "          A     B\n",
      "    0  True  True\n",
      "    1  True  True\n",
      "    2  True  True\n",
      "    3  True  True\n",
      "    4  True  True\n",
      "    >>> df.where(m, -df) == df.mask(~m, -df)\n",
      "          A     B\n",
      "    0  True  True\n",
      "    1  True  True\n",
      "    2  True  True\n",
      "    3  True  True\n",
      "    4  True  True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.DataFrame.where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a112886ef3b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m              .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_approach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "%debug\n",
    "def first_approach():\n",
    "    global df\n",
    "    return (df.where(df['SUMLEV']==50)\n",
    "             .dropna()\n",
    "             .set_index(['STNAME','CTYNAME'])\n",
    "             .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))\n",
    "    \n",
    "timeit.timeit(first_approach, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.where(df5['VI'].notna()).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.notna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.argsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pd.Series().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.MultiIndex.from_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(pd.Series.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "China                 0.000093\n",
    "United States         0.000298\n",
    "Japan                 0.000238\n",
    "United Kingdom        0.000319\n",
    "Russian Federation    0.000128\n",
    "Canada                0.000500\n",
    "Germany               0.000209\n",
    "India                 0.000012\n",
    "France                0.000203\n",
    "South Korea           0.000239\n",
    "Italy                 0.000180\n",
    "Spain                 0.000201\n",
    "Iran                  0.000114\n",
    "Australia             0.000374\n",
    "Brazil                0.000042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_gen = zip([1,2,3,4,5],[6,7,8,9,10])\n",
    "list(zip_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(*zip_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([1,6],[2,7],[3,8],[4,9],[5,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': ['2014-11-14','2009-11-13','2014-09-16','2017-11-15','2014-11-10','2014-10-17','2014-11-18'], 'b': ['l','h','l','h','l','h','h'], 'c': [9,8,7,6,5,4,3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['a']:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a'] = pd.to_datetime(df['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['a'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "df[df['a'].dt.strftime('%Y') > 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "help(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = '2014-11-14'\n",
    "date = pd.to_datetime(data)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAo/ElEQVR4nO3deXxU5b3H8c8vCySAskhEBBRUvCpWUVPEFdkUcEHcQFEBUcRqq9el1drbanvb2mtbW1s3BBGKgIiKqLggUsENDcgiuOFWQYS4scia5Ll/PCcwCZNkkszMmZl836/XvHLmLHO+OTP55cxzznmOOecQEZHMkhV2ABERiT8VdxGRDKTiLiKSgVTcRUQykIq7iEgGygk7AEDr1q1dx44dw44hIpJWFi5c+LVzriDatJQo7h07dqSoqCjsGCIiacXMPq9qmpplREQykIq7iEgGqrG4m1memb1lZkvMbLmZ3R6Mf9jMPjWzxcGjazDezOxuM1tpZkvN7OgE/w4iIlJJLG3u24BezrlNZpYLvGpmzwXTbnLOTa80f3+gc/A4Frgv+CkiIklS45678zYFT3ODR3Ud0gwEJgbLvQm0MLO29Y8qIiKxiqnN3cyyzWwxsA6Y7ZxbEEz6fdD0cpeZNQ7GtQO+iFh8VTCu8muOMrMiMysqLi6u+28gIiK7iam4O+dKnXNdgfZANzM7HLgFOAT4MdAK+EVtVuycG+OcK3TOFRYURD1NU0RE6qhWZ8s4574H5gL9nHNrgqaXbcB4oFsw22qgQ8Ri7YNxIiJSrqwMXrkT1ixNyMvHcrZMgZm1CIbzgb7A++Xt6GZmwNnAu8EiM4FLg7NmugPrnXNrEpBdRCQ9bd8Mj18Gc/8X3n08IauI5WyZtsAEM8vG/zOY5px7xsxeNrMCwIDFwOhg/lnAAGAlsBkYEffUIiLpauNXMOVC+PId6PtbOP5nCVlNjcXdObcUOCrK+F5VzO+Aq+sfTUQkw6xZApOHwNb1MGQyHDIgYatKib5lREQy3oqZ8OSVkN8KRr4A+/wooatT9wMiIonkHMz/C0y7BPY+DK54OeGFHbTnLiKSOCXbYObPYOlUOPw8GHgP5OYlZdUq7iIiibCpGB4dCl8sgJ6/gpNvBLOkrV7FXUQk3tYu9wdOfyiG8x+GLoOSHkHFXUQknj58AaZfBo2awYhZ0C6cjnF1QFVEJB6cg9f/CZMHw14Hwqi5oRV20J67iEj9lWyHWTfAoolw6Jkw6AFo1DTUSCruIiL1sflbmHYpfDYfTroRet4KWeE3iqR/cXcuqUegRUR2Kv4QpgyG9atg0Bg4cnDYiXYK/99Lfax7Dx46Db5eGXYSEWloPn4ZxvaBbRth+LMpVdgh3Yv7pnXw9Ycwpgcsq3y3PxGRBHnrQZh0HjRv76847dCt5mWSLL2L+wE9YPSr0KYLPD4Snr4WdmwJO5WIZKrSEnj2Rph1I3Tu6/uIabFf2KmiSu/iDv4/5/Bn4YTrYOHD/mvS1x+FnUpEMs2W72Hy+fD2g3D8T32vjo33CDtVldK/uANk50Lf22HodNjwJTzQA5ZOCzuViGSKbz6GcX3h03lw1j/g1P+FrOywU1UrM4p7uc59fTNN2yPgiSvgqWv8HU9EROrqs1dhbG/flcClT8HRl4adKCaZVdwBmreDYc/AidfDO//yb0rxB2GnEpF0tGgiTDwbmhbA5XOg44lhJ4pZ5hV3gOwc6PMbuPhx2LQWxpwCi6eEnUpE0kVZKbxwK8z8KXQ6CUbO9l0KpJHMLO7lDurjm2n2PQpmjIYZV8P2H8JOJSKpbNtGmHoRvPFP6DYKLnoM8luEnarWMru4A+y5L1w6E06+CRY/Ag/2gnXvh51KRFLR9/+BcafBR7NhwJ9hwJ2+JSAN1VjczSzPzN4ysyVmttzMbg/GdzKzBWa20sweNbNGwfjGwfOVwfSOCf4dapadA71+BZc8AT98DQ/2hHceCTuViKSS/yzwO3/rV8HF06HbFWEnqpdY9ty3Ab2cc0cCXYF+ZtYd+BNwl3PuIOA7YGQw/0jgu2D8XcF8qeHAXnDVa9DuGHjqJ/DkaDXTiIg/dXrCGb4P9stf8rUizdVY3J23KXiaGzwc0Asov+Z/AnB2MDwweE4wvbdZCvXstcc+/nSmHjfDkqkwpiesXRF2KhEJQ1kZzPmdP3W6w7G+K4GCg8NOFRcxtbmbWbaZLQbWAbOBj4HvnXMlwSyrgHbBcDvgC4Bg+npgryivOcrMisysqLi4uF6/RK1lZUPPW+DSGbDlO/9VbNFE38OkiDQM23+Ax4bB/D/7c9cvfgKatAo7VdzEVNydc6XOua5Ae6AbcEh9V+ycG+OcK3TOFRYUFNT35ermgFP82TQdfuxPeXrySti2qcbFRCTNbfgSxveH956G0/4AZ94NOY3CThVXtTpbxjn3PTAXOA5oYWblh5HbA6uD4dVAB4BgenPgm3iETYg92sAlM+CUX8Kyx/w58WuXh51KRBJl9SLfHPvNx3DRo3Dc1Rl5T4hYzpYpMLMWwXA+0Bd4D1/kzwtmGwY8FQzPDJ4TTH/ZuRRv78jKhlN+4dvit23wzTQLH1YzjUimWT4Dxg+A7EYw8kU4+LSwEyVMLHvubYG5ZrYUeBuY7Zx7BvgFcL2ZrcS3qY8L5h8H7BWMvx64Of6xE6TTyb6ZZr/uvvvgJ67wFzSISHpzDl6507extz3CHzht0yXsVAllqbBTXVhY6IqKisKOsUtZKcz/K/z7D9DqADj/YdjnR2GnEpG62LEVZl7jm12PGAJn/h1y88JOFRdmttA5VxhtWuZfoVoXWdnQ4yYY9rQ/wPpgbyh6SM00Iulm41p4+HRf2Hv/GgbdnzGFvSYq7tXpeKJvpul4Ajzz3zD9Mti6IexUIhKLr5YF3Y2sgAv+BSfdkJEHTqui4l6TZgUw9HH/X3/FDH+/1jVLwk4lItV5f5bvI8aVwYjn4LCzwk6UdCruscjK8v/1hz/r2+/G9oG3x6qZRiTVOAev/d336lhwsD9wum/XsFOFQsW9NvY/HkbP92fVPHsDPDYctq4PO5WIAJRs93dfm/1r6HI2DJ8Fe7YNO1VoVNxrq2lr379zn9v81W0PnAxfvhN2KpGG7YdvYOJAWDzJ9xt13nho1CTsVKFSca+LrCw48b9hxCwo3QHjToUFY9RMIxKGde/7brxXL4Rzx/l+oxrQgdOqqLjXx37d/dk0B/SE526CaZfAlu/DTiXScHz0EozrCzu2+J2tH51X8zINhIp7fTVpBRdOhb6/gw+e8800qxeFnUokszkHCx6AyedDi/1h1FxoH/VangZLxT0esrLghJ/5U67KSn0zzZv3qZlGJBFKd8Cz18NzP4eD+8Nlz0Pz9mGnSjkq7vHUoZs/m+agPvD8zfDoxb6/eBGJjy3fwaRz/RXjJ1wHgydB42Zhp0pJKu7x1qQVXDgFTv09fPi8b6ZZtTDsVCLp75uP/TUmn78OA++Fvrf7b80SlbZMIpjB8dfAZS/4GxI+dCq8cY+aaUTq6pNXfFcCW77zfT4dNTTsRClPxT2R2hfC6HnQ+TR44Zf+qrnN34adSiS9FI2HSef4+x9fPgf2Py7sRGlBxT3R8lvCkEeg3x3w0WzfTPPF22GnEkl9ZaXw3M3wzHX+lpgjX4RWncJOlTZU3JPBDLpfBSNf8MPj+8Hr/1AzjUhVtm6AyYNhwX3Q/Sdw4aOQ1zzsVGlFxT2Z2h0DV86Hg/vBi7+CKUPUTCNS2Xef+QuTPpkLZ9wF/f4I2Tk1LiYVqbgnW34Lf/pW//+DlXPg/pPgPwvCTiWSGj5/wx843fgVXPwEFF4WdqK0peIeBjM49krfhpidA+P7w6t/g7KysJOJhGfxZJh4lj9OdfkcOKBH2InSmop7mNodDVfOg0PPgJd+A5Mv8L3biTQkZWUw+zcw4yrfX9PlL0Hrg8JOlfZqLO5m1sHM5prZCjNbbmbXBuNvM7PVZrY4eAyIWOYWM1tpZh+Y2WmJ/AXSXl5zOH8CDPgzfPoK3H+i/2oq0hBs2+Q73Hvtb3DMCN8Uk98y7FQZIZY99xLgBufcYUB34GozOyyYdpdzrmvwmAUQTBsCdAH6AfeaWXYCsmcOM+h2BYycDTmN/Q195/9VzTSS2davgof6wQezoN+f/MHT7NywU2WMGou7c26Nc25RMLwReA9oV80iA4GpzrltzrlPgZVAt3iEzXj7dvXNNIedBXNu9z3e/fB12KlE4m/VQn/g9PvP/c1vuo9WH+xxVqs2dzPrCBwFlJ/ecY2ZLTWzh8ys/LtUO+CLiMVWEeWfgZmNMrMiMysqLi6uffJMlbenv4vM6X+FT+f7ZprPXgs7lUj8LJsODw+AnDz/bbVzn7ATZaSYi7uZNQMeB65zzm0A7gMOBLoCa4C/1GbFzrkxzrlC51xhQUFBbRbNfGbw45H+wFJuE5hwBsy7U800kt6cg7l/gMdHwr5H+ZtX731I2KkyVkzF3cxy8YX9EefcEwDOubXOuVLnXBnwILuaXlYDHSIWbx+Mk9pqewRc+Qp0OQde/l945FzYpG85koZ2bIHpI+CVP0HXoXDpU/5+xJIwsZwtY8A44D3n3F8jxkfeVnwQ8G4wPBMYYmaNzawT0Bl4K36RG5jGe8C5Y+HMv/vmmftP9M01Iuli41cwfgAsnwF9fwsD7/EnDkhCxXJN7wnAJcAyM1scjPslcKGZdcV3avsZcCWAc265mU0DVuDPtLnaOVca39gNjBkcM9x3X/DYcH+hxym3wEk3QJZORJIUtmYJTB4CW9f7DvQOOT3sRA2GuRTovKqwsNAVFRWFHSM9bNsIz1wPy6ZBpx5+r77Z3mGnEtnde0/DE6MgvxVcNBX2+VHYiTKOmS10zkW9eayuUE03jfeAc8bAWf+ALxb4ZppPXgk7lcguzsH8v/jbTO59mD9wqsKedCru6cgMjr7U/9HkNYeJA2HuH33/1yJhKtnmuxGY81s4/DwY/gzs0SbsVA2Sins6a9MFrpgLRwyGV+7wRX7j2rBTSUO1qRgmnAVLpkDPW32TYW5+2KkaLBX3dNe4GQy635+BsKoI7j8BPp4bdippaNaugLG9/AHU8x+GHj/XFachU3HPBGZw1MUwaq4/ePWvQfDy79VMI8nx4Qv+5hol22HELOgyKOxEgop7Ztn7UF/gu14E8/7Pf0XesCbsVJKpnIM37vF3FGt1gD8G1O7osFNJQMU90zRqCmffC2ffB18u8mfTrJwTdirJNCXb4elr4YVf+nPXL3semlfXn6Akm4p7pup6kT/Y2rQAJp0Lc34HpSVhp5JMsPlbmHQOLJrgL6Q7f6LfqZCUouKeyfY+xH9VPmoozP8zTDgTNnwZdipJZ8Ufwtje/hqLQWOg968hS2UkFeldyXSNmvgzaQaN8Wcy3H8ifPRS2KkkHX38Mozt46+SHv4sHDk47ERSDRX3huLIwTDq39Csje9d8qXb1EwjsXvrQZh0nm9Xv+Jl6KD776Q6FfeGpOBg/4d59DB49S7fT/x69cYs1SgtgVk3wawboXNfGPkitNgv7FQSAxX3hiY3H866G84ZC18t8800H74YdipJRVu+97d6fGsMHHcNDJns+zaStKDi3lAdcT6MegX2bOf/gGf/Gkp3hJ1KUsW3n/gLkz6d5zupO+336l46zai4N2StD4LLZ8MxI+C1v8PDp8P3X9S8nGS2z171N6/+oRgumeE7qZO0o+Le0OXmw5l/g3PH+f5BHjgJPng+7FQSlkX/goln++sjLp8DnU4KO5HUkYq7eD86z9+vtXl7mDIYXrhVzTQNSVkpvPgrmHmNL+gjZ8NeB4adSupBxV122etAGPkS/PhyeOOfML4/fP+fsFNJom3bCFOHwuv/gB9fARc9Bvktwk4l9aTiLhXl5sHpf4HzxsO69+H+k+D9Z8NOJYny/X9g3Gnw0Ysw4M9w+p8hO5ZbK0uqq7G4m1kHM5trZivMbLmZXRuMb2Vms83so+Bny2C8mdndZrbSzJaambqJS0eHn+ObaVruD1Mvgud/6TuLkszxxVv+wOn6VTD0Meh2RdiJJI5i2XMvAW5wzh0GdAeuNrPDgJuBOc65zsCc4DlAf6Bz8BgF3Bf31JIcex3o2167jYI374Hx/eC7z8NOJfGwdJo/O6pRM7j8JTiod9iJJM5qLO7OuTXOuUXB8EbgPaAdMBCYEMw2ATg7GB4ITHTem0ALM2sb7+CSJDmNYcCdcMFE+PojfzbNe8+EnUrqqqzM9xD6xBXQvpu/Yrng4LBTSQLUqs3dzDoCRwELgDbOufI7QXwFlN8Ftx0QebL0qmCcpLPDBsKV8/xNGR4dCs/drGaadLP9B3hsmO8h9OhL4ZInoUmrsFNJgsRc3M2sGfA4cJ1zbkPkNOecA1xtVmxmo8ysyMyKiouLa7OohKVVJ7jsBTh2NCy4Dx46Fb79NOxUEosNX/qzn957Gk79PZx5N+Q0CjuVJFBMxd3McvGF/RHn3BPB6LXlzS3Bz3XB+NVAh4jF2wfjKnDOjXHOFTrnCgsKCuqaX5ItpzH0/xMMngTffAIPnAwrngo7lVTny3f8gdNvPoYLp8Lx1+jm1Q1ALGfLGDAOeM8599eISTOBYcHwMOCpiPGXBmfNdAfWRzTfSKY49EwYPQ/2OgimXep7DizZFnYqqWz5DHioP2Tl+h4d/6tf2IkkSWLZcz8BuAToZWaLg8cA4A6gr5l9BPQJngPMAj4BVgIPAj+Jf2xJCS07+maa7lf7ngPHneo7nJLwOQev3Onb2Nse4Q+ctukSdipJIvPN5eEqLCx0RUVFYceQ+nj/WZhxlS8qZ90NXQaFnajh2rHVdyOw7DE4YrBvX8/NCzuVJICZLXTOFUabpitUJT4OOR1GvwqtD4bHhsOzN/giI8m1ca0/f33ZY9Drf2DQAyrsDZSKu8RPi/1gxHP+xg5vj4VxffxBPEmOr971B07XrYAL/gUn36gDpw2YirvEV04jf2OHC6f6y9of6AHLpoedKvO9P8sf83Bl/h/sYWeFnUhCpuIuifFf/eHK+bD3ofD4SHj6OtixJexUmcc5f6OVqRftukfuvl3DTiUpQMVdEqdFBxgxC064FhaOh7F94euVYafKHCXb4alr/C0Su5wNw2fBnurpQzwVd0ms7Fzo+1vfR/iG1TCmByx9LOxU6e+Hb2DiQFg8CXr8As59CBo1CTuVpBAVd0mOg0+F0fOhzeHwxOUw82dqpqmrde/D2F6weqG/PWLPX0KW/pSlIn0iJHmat4fhz8CJ/w2LJsCDvaH4w7BTpZePXoJxfWH7Zt/k9aPzwk4kKUrFXZIrOxf63AZDH4dNX8GYU2DJo2GnSn3OwYIHYPL50GJ/f+C0fdRrV0QAFXcJS+c+/qKntkfCk6P8gcHtm8NOlZpKd/iLwp77ORzcHy573h+sFqmGiruEZ899YdjTcNKN8M4kGNsbij8IO1Vq2fIdTDoXisb5s44GT4LGzcJOJWlAxV3ClZ0Dvf8HLn4cNq3zzTSLp4SdKjV88zGM7QOfvw4D7/VnHenAqcRInxRJDQf19s00+x4NM0bDjJ/4Owc1VJ/O810JbP4Whs2Eo4aGnUjSjIq7pI4928KlT8HJP4fFk4N+Ut4LO1XyFY2Hfw2CPfbxB073Pz7sRJKGVNwltWTnQK9b/f09N38DY3rCO4+EnSo5ykrh+VvgmevggFP8zTVadQo7laQpFXdJTQf29M007QvhqZ/Ak6Nh26awUyXO1g0weTC8eS8cexVc+CjkNQ87laQxFXdJXXvs45tpetwMS6bCgz1h7YqwU8Xfd5/5Hh0/mQtn3AX97/DfYETqQcVdUltWNvS8xRf5Ld/7Ar9oor+oJxN8/oY/trDxS7j4CSi8LOxEkiFU3CU9HNDDN9N0OBZm/hSeGJX+zTSLp8DEsyCvBVz+sv8dReJExV3Sxx5t/IHWnrfCu9N9D5NfvRt2qtorK4OXbvOnfO7XHa6YA60PCjuVZJgai7uZPWRm68zs3Yhxt5nZajNbHDwGREy7xcxWmtkHZnZaooJLA5WVDT1+DpfOhG0b/VWtRePTp5lm2yaYdgm8ehccM8I3xeS3DDuVZKBY9twfBvpFGX+Xc65r8JgFYGaHAUOALsEy95pZdrzCiuzU6SQY/Rrsd5w/dfDxy32xT2XrV8H4fvDBLOj3J3/wNDs37FSSoWos7s65ecC3Mb7eQGCqc26bc+5TYCXQrR75RKrWrMDv+fb6H1j+hL9f65qlYaeKbtVCf+D028/gomnQfbRuXi0JVZ8292vMbGnQbFP+vbId8EXEPKuCcbsxs1FmVmRmRcXFxfWIIQ1aVhacfCMMewZ2bPZ9sbw9LrWaaZZNh4cHQE4eXD4bOvcNO5E0AHUt7vcBBwJdgTXAX2r7As65Mc65QudcYUFBQR1jiAQ6nuDPpul4Ijx7PUwf4S8MCpNzMPeP/gbh+x7luxLY+9BwM0mDUafi7pxb65wrdc6VAQ+yq+llNRDZ0XT7YJxI4jVtDUOnQ+/fwIqZ8MDJsGZJOFl2bPH/YF65A468yJ+n37R1OFmkQapTcTezyFusDwLKz6SZCQwxs8Zm1gnoDLxVv4gitZCVBSddD8OfhZJtvpnmrQeT20yz8SsYPwCWz4A+t8PZ90JO4+StXwSo8RpnM5sCnAK0NrNVwG+AU8ysK+CAz4ArAZxzy81sGrACKAGuds6VJiS5SHX2P84308wYDbNuhM/mw1n/SHx/LWuWwJQL/dW0Qx6BQ05P7PpEqmAuBQ48FRYWuqKiorBjSCYqK4PX74Y5v/W3pjv/Yd/+nQjvPe2vnM1vBRdOgbZHJGY9IgEzW+ici3ozXV2hKpktKwtOvA5GPAelJb6DrgUPxLeZxjmY/1d49GLY+zB/4FSFXUKm4i4Nw37Hwuj5cEBPf6PpaZf4ppP6KtkGM66CObfD4efB8Gd8NwkiIVNxl4ajSSu4cCr0/R188Jw/m2b1wrq/3g9fw4SzYMkU39/NuWMhNz9+eUXqQcVdGpasLDjhZzDieXBlMO40ePO+2jfTrF3hux9esxjOG+/7u9EVp5JCVNylYerwY7hynr9a9PmbfXv5lu9iW/bDF33bfcl2GDELDj8nsVlF6kDFXRquJq1gyGQ47Q/w4fNw/8mwqpqztpyDN+6BKYP9vU2veBnaHZO8vCK1oOIuDZsZHHc1XPaCf/7QafD6P3dvpinZDk9fCy/80p+7ftnz0Dxqt0kiKUHFXQT8jbhHz4OD+8GLt/oLkTYHnaFu/hYmnQOLJsBJN8D5E6FR03DzitRAxV2kXH5LGDzJ97W+8iV/Ns2y6f6GIF8sgEFjoPev/UFZkRSnT6lIJDPf1/rIF8CyfI+OWzf4LoWPHBx2OpGY1di3jEiD1O4YfzZN0Th/cVLL/cNOJFIrKu4iVclv4dvYRdKQmmVERDKQiruISAZScRcRyUAq7iIiGUjFXUQkA6m4i4hkIBV3EZEMpOIuIpKBaizuZvaQma0zs3cjxrUys9lm9lHws2Uw3szsbjNbaWZLzezoRIYXEZHoYtlzfxjoV2nczcAc51xnYE7wHKA/0Dl4jALui09MERGpjRqLu3NuHvBtpdEDgQnB8ATg7IjxE533JtDCzNrGKauIiMSorm3ubZxza4Lhr4Dy2723A76ImG9VMG43ZjbKzIrMrKi4uLiOMUREJJp6H1B1zjmglncXBufcGOdcoXOusKCgoL4xREQkQl2L+9ry5pbg57pg/GqgQ8R87YNxIiKSRHUt7jOBYcHwMOCpiPGXBmfNdAfWRzTfiIhIktTYn7uZTQFOAVqb2SrgN8AdwDQzGwl8DlwQzD4LGACsBDYDIxKQWUREalBjcXfOXVjFpN5R5nXA1fUNJSIi9aMrVEVEMpCKu4hIBlJxFxHJQCruIiIZSMVdRCQDqbiLiGQgFXcRkQyk4i4ikoFU3EVEMpCKu4hIBlJxFxHJQCruIiIZSMVdRCQDqbiLiGQgFXcRkQyk4i4ikoFU3EVEMpCKu4hIBlJxFxHJQCruIiIZqMYbZFfHzD4DNgKlQIlzrtDMWgGPAh2Bz4ALnHPf1S+miIjURjz23Hs657o65wqD5zcDc5xznYE5wXMREUmiRDTLDAQmBMMTgLMTsA4REalGfYu7A140s4VmNioY18Y5tyYY/gpoE21BMxtlZkVmVlRcXFzPGCIiEqlebe7Aic651Wa2NzDbzN6PnOicc2bmoi3onBsDjAEoLCyMOo+IiNRNvfbcnXOrg5/rgCeBbsBaM2sLEPxcV9+QIiJSO3Uu7mbW1Mz2KB8GTgXeBWYCw4LZhgFP1TekiIjUTn2aZdoAT5pZ+etMds49b2ZvA9PMbCTwOXBB/WOKiEht1Lm4O+c+AY6MMv4boHd9QomISP3oClURkQyk4i4ikoHqeyqkiEiDUubK2Fqyla2lW9lSsoUtO7bsGq702FqydefPzSWbK4wrHz7jwDMYeujQuOdUcReRjFJefKMW2dKgyFYqyJXnj1aEdw6Xbq11psbZjcnPyScvJ4/8nHw/nJ1H87zmNM1tmoCtoOIuIklWWlbKttJtVe7J7lZkS7eyZUfEcMkWNpdsrrBXHLl8XYpvXnZeheJb/rNlXkv2zdm3QkHOz40YDsbvnB5ZvHPyaJLThMbZjcnOyk7AlqyeiruIVFBaVlqhyWFLafQ92WqLc+mW3Qpy+fRtpdtqnalyMS0vonvl7VWhoFZXZCNfo/L0LMu8w48q7iJpprSsNHqRjbKHG0szQ+U95O1l22uVx7DdCmz5Hm7r/NbRi2zO7kW2quLcOLtxRhbfRFNxF4mzkrKS2NtyS7eyecfmCgU5WhGObIbYUbajVnmiFd/yR0GTgop7xbn55GdXvwdceVxedh7BxYySQlTcpcHZUbYjpmaGaIU5WnGuPL2krKRWeQzbrZiWF9E9m+xZdZGN1v4bUZzLl2mc3VjFtwFScZeUs6N0R/Q222raf2sswhFtwbUtvlmWFfWAWn52Pi0at9it8FbXzFCheAev1SirkYqvxJ2Ku9SKc46SspJqD6ZtLtm8s/228sG06pYr/1niald8sy27yiLaIq+F35OttIcbS1tv+XMVX0lHKu4Zxjm3s9mhqr3YapsXIvaOqyrCpa60VplyLCdq00J+Tj6t8lpFb8uttIdc1fJNcpqQk5Wj4itSiYp7kjnn2F62PeqBstpeTLGzCO+oeDCu1sU3K2dXEc3dtXfbNKcpe+XtVeNpZlH3gMuLc3Y+udm5CdqaIlIVFfdKyotveXtvtAJamyJc4WKLYPkyV1arTLlZubsdaMvPyadZbjNa57WuUJCra+utfBpa+XBuloqvSKZJ6+K+ecdmvvrhqyrP1915qXEtDsBtLd1a6+LbKKtR1D3YZo2a7TrVLNqFFlEuyohWmHOy0vptEpEQpHXVmLdqHjfNu6nG+RplNYp6yfCejfakTZM2tTvbITvYAw72lvNy8lR8RSTlpHVVOrLgSO48+c5qi3Nedl4o/TqIiIQprYt722ZtadusbdgxRERSjjpsEBHJQAkr7mbWz8w+MLOVZnZzotYjIiK7S0izjJllA/cAfYFVwNtmNtM5tyIR60snzjmcA1c+DMFzP55KzyvPRzXTnJ9Y8fnO4V3rrnEdFcZHzFebnLutu+aslV+H3dYdY86Ibb1bzurWEe11IjNXyF/x9YM0UV9n9/e9mnVEeQ2irK/KdVTxOju3VQzr2LX9ano/qljHblmqfi8qb7fqP99VbKvq1hE5fufrV7GtarMdK88X6zqi/I5XnnwAP+93CPGWqDb3bsBK59wnAGY2FRgIxLW4//uDdfzumRUxv2lUNZ2a37i6vGmV/1BEqmIGBphZ8NN3KMbO8f55+YW4kfMSuWyU14HI8btep3y+neuvNI2q1hEtT+T4yq9TYZkov2Pk62eBkbXb6+y2jlh/x2jrqOJ12G3bVHydWLYjUadVv45unfaqwyemZokq7u2ALyKerwKOjZzBzEYBowD222+/Oq1kj7xcDtlnz5jeNP+jfm9cbd+0yn+MFT/ku79Oleuo6QMe9Y+ociGgUrGI/se4+zoqZqm4nXZ/jcjXr7zuXTl3374xraNCxt3/gCu+Ri1ep4rfN2rOytuyht83+nsaud12hhWJq9DOlnHOjQHGABQWFtZpv/aY/VtyzP4t45pLRCQTJOqA6mqgQ8Tz9sE4ERFJgkQV97eBzmbWycwaAUOAmQlal4iIVJKQZhnnXImZXQO8AGQDDznnlidiXSIisruEtbk752YBsxL1+iIiUjVdoSoikoFU3EVEMpCKu4hIBlJxFxHJQOZS4Lp4MysGPq/j4q2Br+MYJ15SNRekbjblqh3lqp1MzLW/c64g2oSUKO71YWZFzrnCsHNUlqq5IHWzKVftKFftNLRcapYREclAKu4iIhkoE4r7mLADVCFVc0HqZlOu2lGu2mlQudK+zV1ERHaXCXvuIiJSiYq7iEgGSuniXtNNts2ssZk9GkxfYGYdI6bdEoz/wMxOS3Ku681shZktNbM5ZrZ/xLRSM1scPOLaDXIMuYabWXHE+i+PmDbMzD4KHsOSnOuuiEwfmtn3EdMSub0eMrN1ZvZuFdPNzO4Oci81s6MjpiVye9WUa2iQZ5mZvW5mR0ZM+ywYv9jMipKc6xQzWx/xfv06Ylq1n4EE57opItO7wWeqVTAtIdvLzDqY2dygDiw3s2ujzJPYz5e/cW/qPfBdBX8MHAA0ApYAh1Wa5yfA/cHwEODRYPiwYP7GQKfgdbKTmKsn0CQYvqo8V/B8U4jbazjwzyjLtgI+CX62DIZbJitXpfl/iu8iOqHbK3jtk4GjgXermD4AeA5/d7zuwIJEb68Ycx1fvj6gf3mu4PlnQOuQttcpwDP1/QzEO1elec8EXk709gLaAkcHw3sAH0b5e0zo5yuV99x33mTbObcdKL/JdqSBwIRgeDrQ28wsGD/VObfNOfcpsDJ4vaTkcs7Ndc5tDp6+ib8TVaLFsr2qchow2zn3rXPuO2A20C+kXBcCU+K07mo55+YB31Yzy0BgovPeBFqYWVsSu71qzOWcez1YLyTv8xXL9qpKfT6b8c6VlM+Xc26Nc25RMLwReA9/b+lICf18pXJxj3aT7cobZ+c8zrkSYD2wV4zLJjJXpJH4/87l8sysyMzeNLOz45SpNrnODb4CTjez8lshpsT2CpqvOgEvR4xO1PaKRVXZE7m9aqvy58sBL5rZQvM3oU+248xsiZk9Z2ZdgnEpsb3MrAm+SD4eMTrh28t8c/FRwIJKkxL6+QrtBtkNgZldDBQCPSJG7++cW21mBwAvm9ky59zHSYr0NDDFObfNzK7Ef+vplaR1x2IIMN05VxoxLsztldLMrCe+uJ8YMfrEYHvtDcw2s/eDPdtkWIR/vzaZ2QBgBtA5SeuOxZnAa865yL38hG4vM2uG/2dynXNuQ7xeNxapvOcey022d85jZjlAc+CbGJdNZC7MrA9wK3CWc25b+Xjn3Org5yfAv/H/0ZOSyzn3TUSWscAxsS6byFwRhlDpK3MCt1csqsoe+g3gzewI/Hs40Dn3Tfn4iO21DniS+DVH1sg5t8E5tykYngXkmllrUmB7Bar7fMV9e5lZLr6wP+KceyLKLIn9fMX7QEK8HvhvFZ/gv6aXH4TpUmmeq6l4QHVaMNyFigdUPyF+B1RjyXUU/gBS50rjWwKNg+HWwEfE6cBSjLnaRgwPAt50uw7gfBrkaxkMt0pWrmC+Q/AHtywZ2ytiHR2p+gDh6VQ84PVWordXjLn2wx9HOr7S+KbAHhHDrwP9kphrn/L3D18k/xNsu5g+A4nKFUxvjm+Xb5qM7RX83hOBv1UzT0I/X3HbuIl44I8mf4gvlLcG436L3xsGyAMeCz7obwEHRCx7a7DcB0D/JOd6CVgLLA4eM4PxxwPLgg/3MmBkknP9EVgerH8ucEjEspcF23ElMCKZuYLntwF3VFou0dtrCrAG2IFv1xwJjAZGB9MNuCfIvQwoTNL2qinXWOC7iM9XUTD+gGBbLQne51uTnOuaiM/Xm0T884n2GUhWrmCe4fiTLCKXS9j2wjeVOWBpxPs0IJmfL3U/ICKSgVK5zV1EROpIxV1EJAOpuIuIZCAVdxGRDKTiLiKSgVTcRUQykIq7iEgG+n8jnzIUmQ4lywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "x = pd.DataFrame({'a': [1,2,3], 'b': [321,123,345] , 'c': [12,23,34]})\n",
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-21-fe42bc865289>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_dic_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> exit()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a\n",
       "0  1\n",
       "1  2\n",
       "2  3\n",
       "3  4\n",
       "4  4\n",
       "5  4\n",
       "6  4\n",
       "7  4\n",
       "8  5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%debug\n",
    "import pandas as pd\n",
    "pd.DataFrame({'a': [1,2,3,4,4,4,4,4,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "655px",
    "left": "8px",
    "right": "20px",
    "top": "360px",
    "width": "262px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
